### **LightGBM を用いた格付け変化の予測式**

時点 $t = 0$ における債権 $i$ の格付けを $R_i(0)$ とする。  
この格付けは時点 $t$ において下落幅 $\Delta_i(t)$ を持ち、最終的な格付け $R_i(t)$ は以下で与えられる：

$$
R_i(t) = \max \left( R_{\min}, R_i(0) - \Delta_i(t) \right)
$$

#### **LightGBM を用いた下落幅の予測**
下落幅 $\Delta_i(t)$ は LightGBM モデル $f_{\text{LGB}}$ によって予測されるとし、以下のように表せる：

$$
\Delta_i(t) = f_{\text{LGB}} (X_i)
$$

ここで、
- $X_i$ : 債権 $i$ に関する特徴量ベクトル
- $f_{\text{LGB}}$ : LightGBM による予測モデル

したがって、最終的な格付けの予測式は

$$
R_i(t) = \max \left( R_{\min}, R_i(0) - f_{\text{LGB}} (X_i) \right)
$$







ドキュメンテーション認識合わせ
0. 課題
　・実装ロジックについては説明資料が存在するもの/しないもの、また存在する場合にもドキュメントが最新化されているか整理されていないため、新規着任者や引き継ぎを受ける場合においてコードを理解するのが難しい
　・また現在のチームメンバーにおいても各自の対応部分以外の実装内容への理解が深くは進んでおらず、引き継ぎが困難な状況
1. 目的
　・コード理解の促進
　　コード全体の詳細なロジックを把握することはドキュメントがない場合は困難。ドキュメンテーションを整理し更新状況を管理することで引き継ぎ時など効率的に理解を深められるようにする
　・知識の属人化の解消
　　ある程度の時間をかけて他人のコードを理解し、それをドキュメンテーション化することで、チーム内の知識の属人化を防ぐ
　・コードの品質向上
　　実装者、実装者以外の両方の視点でコードが何をしているのか、なぜそうするのかを明確にすることで、コード自体の改善点や最適化を明らかにする
　　他人のコードを読むことで、新しいテクニックやアプローチを学ぶ

 既存ドキュメントとの立ち位置の整理
　既存ドキュメント
　　


ドキュメンテーションの構成、範囲について
①主要な関数/メソッドの説明: スクリプトの中心的な機能を果たす主要な関数やメソッドを説明する。その機能、入力と出力、副作用（あれば）を明確にする
②複雑なロジックの説明: に複雑なアルゴリズムやロジック、またはプロジェクト全体に影響を与える重要な部分についての詳細な説明を記載する
③エラーハンドリングと特殊なケース: コードがどのようにエラーを処理し、特殊なケースを扱うのかを説明
④変更履歴: 変更履歴や更新履歴を追加することで、コードがどのように進化し、どのような修正が行われたのかを把握する
⑤既知の問題と未解決の課題: コードに関して既知の問題や制約事項・未解決の課題を明記する



レビューと更新のプロセス
・レビュープロセス
　レビュアー：基本チーム内メンバーで最低限実装者
　レビュー基準
　　明確さと理解の容易さ
　　技術的正確性
　　最新性（ドキュメント更新時点）



更新プロセス
　更新のトリガー
　　何がドキュメンテーションの更新を引き起こすのかを定義
　バージョニング
　　ドキュメンテーションのバージョンを追跡するための方針
　アーカイブ
　　古い、またはもはや必要とされないドキュメンテーションをどのように保管または廃棄するか

import pandas as pd

def replace_ban(df: pd.DataFrame, column: str) -> pd.DataFrame:
    # '{数字}番'を'{数字}-'に変換（ただし、「番」の直後に「地」、「町」、「号」が続く場合は除く）
    df[column] = df[column].str.replace('(\d+)番(?![地町号])', r'\1-', regex=True)
    return df

def prepend_prefecture(df: pd.DataFrame, prefecture_column: str, address_column: str) -> pd.DataFrame:
    df[address_column] = df.apply(lambda row: row[prefecture_column] + row[address_column]
                                  if not row[address_column].startswith(row[prefecture_column]) else row[address_column], axis=1)
    return df

import pandas as pd
import re

def replace_address_format(df: pd.DataFrame, column: str) -> pd.DataFrame:
    # "{数字}丁目{数字}番{数字}号"の形式を"{数字}-{数字}-{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)丁目(\d+)番(\d+)号', r'\1-\2-\3', x))

    # "{数字}丁目{数字}番"の形式を"{数字}-{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)丁目(\d+)番', r'\1-\2', x))

    # "{数字}丁目{数字}号"の形式を"{数字}-{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)丁目(\d+)号', r'\1-\2', x))

    # "{数字}番{数字}号"の形式を"{数字}-{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)番(\d+)号', r'\1-\2', x))

    # "{数字}丁目"の形式を"{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)丁目', r'\1', x))

    # "{数字}番"の形式を"{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)番', r'\1', x))

    # "{数字}号"の形式を"{数字}"に変換
    df[column] = df[column].apply(lambda x: re.sub('(\d+)号', r'\1', x))

    return df


import pandas as pd
import jaconv

def convert_kana(df: pd.DataFrame, column: str) -> pd.DataFrame:
    df[column] = df[column].apply(jaconv.kata2kata, kana_from='H', kana_to='K')
    return df

import pandas as pd

def convert_kana(df: pd.DataFrame, column: str) -> pd.DataFrame:
    # 変換テーブル
    trans_table = str.maketrans('ぁぃぅぇぉっゃゅょゎァィゥェォッャュョヮ', 'あいうえおつやゆよわアイウエオツヤユヨワ')
    # 変換
    df[column] = df[column].apply(lambda x: x.translate(trans_table))
    return df

import pandas as pd
import re

import pandas as pd
import re

def normalize_parentheses(df: pd.DataFrame, column: str) -> pd.DataFrame:
    # ①両括弧（）がある場合：両括弧を削除し、括弧内にあった文字も削除する
    df[column] = df[column].apply(lambda x: re.sub(r'[\(（].*?[\)）]', '', x))
    # ②右括弧)のみある場合：右括弧と前方にスペースが現れるまですべての文字を削除する
    df[column] = df[column].apply(lambda x: re.sub(r'.*[\)）]', '', x))
    # ③左括弧(のみある場合：左括弧と後方にスペースが現れるまですべての文字を削除する
    df[column] = df[column].apply(lambda x: re.sub(r'[\(（].*', '', x))
    return df

def simplify_kana(text: str) -> str:
    trans_map = str.maketrans({
        'ｶﾞ': 'ｶ', 'ｷﾞ': 'ｷ', 'ｸﾞ': 'ｸ', 'ｹﾞ': 'ｹ', 'ｺﾞ': 'ｺ',
        'ｻﾞ': 'ｻ', 'ｼﾞ': 'ｼ', 'ｽﾞ': 'ｽ', 'ｾﾞ': 'ｾ', 'ｿﾞ': 'ｿ',
        'ﾀﾞ': 'ﾀ', 'ﾁﾞ': 'ﾁ', 'ﾂﾞ': 'ﾂ', 'ﾃﾞ': 'ﾃ', 'ﾄﾞ': 'ﾄ',
        'ﾊﾞ': 'ﾊ', 'ﾋﾞ': 'ﾋ', 'ﾌﾞ': 'ﾌ', 'ﾍﾞ': 'ﾍ', 'ﾎﾞ': 'ﾎ',
        'ﾊﾟ': 'ﾊ', 'ﾋﾟ': 'ﾋ', 'ﾌﾟ': 'ﾌ', 'ﾍﾟ': 'ﾍ', 'ﾎﾟ': 'ﾎ',
        'ｧ': 'ｱ', 'ｨ': 'ｲ', 'ｩ': 'ｳ', 'ｪ': 'ｴ', 'ｫ': 'ｵ',
        'ｯ': 'ﾂ', 'ｬ': 'ﾔ', 'ｭ': 'ﾕ', 'ｮ': 'ﾖ',
    })

    return text.translate(trans_map)


text = "ﾊﾟｲｿﾝﾌﾟﾛｸﾞﾗﾐﾝｸﾞｨｩｪｫ"
print(simplify_kana(text))  # ｱｲｿﾝﾌﾛｸﾞﾗﾐﾝｸﾞｲｳｴｵ

def truncate_after_city(address):
    # 政令指定都市のリスト（2021年9月時点）
    designated_cities = [
        "札幌市", "仙台市", "さいたま市", "千葉市", "横浜市", "川崎市", "相模原市",
        "新潟市", "静岡市", "浜松市", "名古屋市", "京都市", "大阪市", "堺市",
        "神戸市", "岡山市", "広島市", "北九州市", "福岡市", "熊本市"
    ]

    for city in designated_cities:
        if city in address:
            # "市"の位置の次までを取得して、それ以降を削除
            idx = address.index(city) + len(city)
            return address[:idx]

    return address  # 政令指定都市が見つからなかった場合は元の住所をそのまま返す

# 例
address = "神奈川県横浜市西区1-2-3"
truncated_address = truncate_after_city(address)
print(truncated_address)  # -> 神奈川県横浜市



import pandas as pd
import numpy as np

# 特徴ベクトルの長さ
vector_length = 768

# データフレームの行数
num_rows = 100000

# ランダムな特徴ベクトルの生成
np.random.seed(42)  # 結果の再現性を保証
feature_vectors_a = [np.random.rand(vector_length).tolist() for _ in range(num_rows)]
feature_vectors_b = [np.random.rand(vector_length).tolist() for _ in range(num_rows)]

# データフレームの作成
df = pd.DataFrame({
    "特徴ベクトルA": feature_vectors_a,
    "特徴ベクトルB": feature_vectors_b
})

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
from concurrent.futures import ProcessPoolExecutor
from tqdm.auto import tqdm
import os

def calculate_similarity_for_subchunk(subchunk_pair):
    vec1, vec2 = subchunk_pair  # 展開
    similarity_matrix = cosine_similarity(vec1, vec2)
    return np.diag(similarity_matrix)

def process_subchunks(subchunks, calc_cols):
    subchunk_pairs = [(np.stack(subchunk[calc_cols[0]].values), np.stack(subchunk[calc_cols[1]].values)) for subchunk in subchunks]
    
    # 利用可能なCPUコア数に基づいてmax_workersを設定
    max_workers = min(len(subchunks), os.cpu_count() or 1)
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # mapを使用してより効率的な並列処理を実行
        results = list(tqdm(executor.map(calculate_similarity_for_subchunk, subchunk_pairs), total=len(subchunks)))
    
    # 結果を結合して返す
    return np.concatenate(results)

def calc_similarity(df, calc_cols=["特徴ベクトルA", "特徴ベクトルB"]):
    df["score"] = 0.0
    chunk_size = 1000  # 大きなチャンクサイズ
    subchunk_size = 100  # サブチャンクサイズ
    
    for start in tqdm(range(0, len(df), chunk_size)):
        end = min(start + chunk_size, len(df))
        chunk = df.iloc[start:end]
        
        # 1000件のチャンクをさらに100件ごとのサブチャンクに分割
        subchunks = [chunk.iloc[i:i+subchunk_size] for i in range(0, len(chunk), subchunk_size)]
        
        chunk_results = process_subchunks(subchunks, calc_cols)
        
        # 結果を元のDataFrameに統合
        df.loc[start:end-1, "score"] = chunk_results
    
    return df


# calc_similarity関数の実行
result_df = calc_similarity(df, ["特徴ベクトルA", "特徴ベクトルB"])

# 結果の確認
result_df.head()
